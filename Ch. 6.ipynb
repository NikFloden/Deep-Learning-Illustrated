{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifical Neurons Detecting Hot Dogs\n",
    "Artificial neurons are based on biological ones (who'da thunk). A biological neuron *recieves information* from other neurons via its dendrites, *aggregates this information* with changes in cell voltage in the cell body, and *transmits a signal if the cell voltage crosses a threshold*, a signal that can be recieved by many other neurons in the network. \n",
    "\n",
    "First artificial neuron was the Perceptron, by neurobiologist Frank Rosenblatt. \n",
    "The perceptron algorithm used weighted inputs, in the example it was the presence of ketchup(3 pts), mustard(2 pts), and bun (6 pts). Determining the weighted sum of the inputs and comparing them to a threshold (theshold was 4 in the example) outputs a 1 (it's a hotdog) or a 0 (it's not a hotdog). \n",
    "\n",
    "Bias. Bias in an artificial neuron is negative threshold value. \n",
    "\n",
    "These values (input weights, threshold, bias, etc) are the parameters of the neuron. \n",
    "\n",
    "## Modern Neurons and Activation Functions\n",
    "Modern artifical neurons are not perceptrons. The most obvious restriction of the perceptron is that it only has a binary output. This makes learning very hard for a perceptron-based network, as well as we want to make predictions from inputs that are continuous variables. \n",
    "\n",
    "## Sigmoid\n",
    "Compared to the step transition of the perceptron's binary output, the sigmoid uses a curve and a function. Sigmoid functions are ubiquitous in statistics (the normal distribution and the t-distribution for example). The sigmoid function is defined by Sigma(Z) = 1/(1+e^(-z)) where z is equivalent to w\\*x+b. \n",
    "The sigmoid function is the first example of an *activation function*. The sigmoid function is the canonical activation function, so much so that Sigma is conventionally used for *any* activation function. The output from any given neuron's activation function is referred to simply as its *activation*, in the book they use the term **a**.\n",
    "\n",
    "Now we work with the Sigmoid Function notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to load the constant *e*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+e**-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000024999999999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As z goes closer to 0, sigma goes closer to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then larger values of z will equal 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.539786870243442e-05"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-1)\n",
    "sigmoid(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values of z will move closer toward zero. \n",
    "\n",
    "Pretty standard stuff in all honesty. Small gradual changes in the neuron's *w* and *b* parameters cause small, gradual changes in *z*, causing gradual changes in the neuron's activation **a**. Large negatives and large positives of z illustrate exception, extreme values create zeroes and ones. Like the perceptron, subtle updates to the weights and biases during training will have little to no effects on the output, so learning will stall. This is called neuron *saturation* and occurs with most activation functions. \n",
    "\n",
    "## Tahn Neuron\n",
    "(Pronounced tanch colloquially)\n",
    "The tahn activation function is defined as Sigma(z) = (e^z - e^(-z))/(e^z + e^(-z)). The shape of the tahn curve is similar to the sigmoid, but the tahn's output is \\[-1;1\\]. Negative z inputs correspond to negative **a** activations, z = 0 corresponds to **a** = 0, positive z inputs correspond to positive **a** activations. The output from tahn neurons tend to be around zero, which reduces neuron saturation. \n",
    "\n",
    "## ReLU: Rectified Linear Units\n",
    "ReLU functions are characterized by **a** = *max*(0,z). If z is a positive value, **a** = z, if z is negative or zero, **a**=0. The output **a** does not vary uniformly linearly across all values of z. The ReLU is in essence two distinct linear functions combined to form a straightforward nonlinear function overall. Nonlinearity is essential to activation functions used within deep learning architectures. Nonlinear functions permit deep learning models to pproximate any continuous function. This univeral ability to approximate some output y given some input x is one of the hallmarks of deep learning - the characteristic that makes the approach so effective across such a breadth of applications. \n",
    "The simple shape of the ReLU's brand of nonlinearity works to its advantage. Learning appropriate values for *w* and *b* within deep learning networks involves partial derivative calculus, and these calculus operations are more computationally efficent on the linear portions of the ReLU relative to its efficency on the curves of, say, the sigmoid and tanh functions. ReLU are the most widely used neurons. \n",
    "\n",
    "## choosing a neuron\n",
    "ReLU is the most preferred. It's very efficent compared to tanh and sigmoid. Don't bother with a perceptron. \n",
    "Several other forms of ReLUs include: Leaky, parametric and exponential linear unit. \n",
    "\n",
    "## Key Concepts of Chapter 6\n",
    "Parameters:\n",
    "    Weight *w*\n",
    "    Bias *b*\n",
    "Activation **a**\n",
    "Artificial neurons:\n",
    "    sigmoid\n",
    "    tanh\n",
    "    ReLu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
